{"cells":[{"cell_type":"markdown","metadata":{"id":"dAP-ZXq7EH-X"},"source":["\n","---\n","### **Generative Adversarial Networks for Lyrics Generation**\n","Raghad Taleb, ID: 210885240\n","\n","in this project, we will test the RNN neural netowrk and markovian model (https://github.com/jsvine/markovify) to generate lyrics songs using data scraped from AZLyrics.\n"]},{"cell_type":"markdown","metadata":{"id":"LMHXcDCNA4RI"},"source":["# Github Repo: https://github.com/raghadt/Generating-Lyrics\n","------"]},{"cell_type":"markdown","metadata":{"id":"gxGQzIBGEH-e"},"source":["## Logistics Code:"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2801,"status":"ok","timestamp":1651260961811,"user":{"displayName":"Raghad Taleb","userId":"09571408088199853670"},"user_tz":-60},"id":"pnZX-Oii_NKN","outputId":"f70dbe19-42de-421e-f9d5-1fc069dbd0b3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found GPU at: \n","2.8.0\n"]}],"source":["#@title Imports { vertical-output: true , form-width: \"30%\" }\n","\n","import tensorflow as tf\n","import numpy as np\n","import random\n","import os\n","import json\n","\n","device_name = tf.test.gpu_device_name()\n","print('Found GPU at: {}'.format(device_name))\n","\n","print(tf.__version__)\n","\n","# from google.colab import drive\n","# drive.mount('/content/gdrive')\n"]},{"cell_type":"markdown","metadata":{"id":"l8TRb5r8XFyj"},"source":["# ⚠ : IMPORTANT: \n","\n","Please download the scraped data named \"LDR_lyrics.json\" by using :\n","\n","```\n","!wget https://raw.githubusercontent.com/raghadt/Generating-Lyrics/master/LDR_lyrics.json\n","```\n","\n","or use the file included in the zip file of the submission folder.\n","\n","or you can run the code yourself, but it's going to take ~3 hours to scrape the songs :) \n","\n","**Importing Data**\n","\n","1- set the path of where your data is"]},{"cell_type":"markdown","metadata":{"id":"HhRcDUy9XC2K"},"source":["## Data Handling\n","- Data Wrangling Code - From AZLYRICS\n","- Data Cleaning"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1651260961811,"user":{"displayName":"Raghad Taleb","userId":"09571408088199853670"},"user_tz":-60},"id":"2E5-5nG5XC2L"},"outputs":[],"source":["# # @title Data Wrangling Code - From AZLYRICS\n","\n","# from bs4 import BeautifulSoup\n","# import requests\n","# import time\n","\n","# # You can change the URL to another artist. Mine was Lana Del Rey\n","# BASE_URL = 'https://www.azlyrics.com/l/lanadelrey.html'\n","# r = requests.get(BASE_URL)\n","# soup = BeautifulSoup(r.text, 'html')\n","\n","# # ============ Get Album names =============\n","# lana_dict={}\n","# lana_dict['album'] = {}\n","# # get albums and store them in lana_dict\n","# albums_name = soup.find_all(\"div\", {\"class\":\"album\"})\n","\n","# for album in albums_name:\n","#     try:\n","#         album_str = (album.select_one('b').get_text().strip('\"'))\n","#         lana_dict[album_str]= {}\n","#     except:\n","#         continue;\n","        \n","\n","    \n","# def get_song_page(url):\n","#     # url = self.get_song_url()\n","#     response = requests.get(url)\n","#     return response.content\n","\n","# # =========== get song url ===================\n","# songs_url = []\n","# songs = soup.find_all(\"div\", {\"class\":\"listalbum-item\"})\n","# for song in songs:\n","#     try:\n","#         songs_url.append('https://www.azlyrics.com'+song.select_one(\"a\").get('href'))\n","#     except:\n","#         print(song)\n","\n","# print(\"Done Creating songs url list\")\n","\n","# # ======= fetch the lyrics from the urls ======\n","\n","# HTML_TAGS = ['br','div', 'i']\n","# lyrics_dictionary = {}\n","# # def get_lyrics(self):\n","# for song_url in songs_url:\n","#     try:\n","#         song_title = song_url.split('/')[-1].replace('.html','')\n","#         soup = BeautifulSoup(get_song_page(song_url), 'html')\n","#         page_lyric = soup.find_all(\"div\", limit=21)[-1]  # since the lyrics start on 22nd div\n","#         lyrics = ''.join(page_lyric.find_all(text=True))\n","\n","#         lyrics = [word for word in lyrics.split(' ') if word not in HTML_TAGS]\n","\n","#         song_lyrics = (\" \".join(lyrics[19:])) #remove the warning\n","\n","#         lyrics_dictionary[song_title] = song_lyrics\n","#         # break\n","#     except:\n","#         print(song_url)\n","#     time.sleep(10) # stop for 10 seconds so it doesn't request too much\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gAYfSbRHkefF"},"source":["**Download Dataset**"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":309,"status":"ok","timestamp":1651260962117,"user":{"displayName":"Raghad Taleb","userId":"09571408088199853670"},"user_tz":-60},"id":"SXqHDbKcYHmM","outputId":"7844db9e-b202-4ee9-e76e-f0de48daed10"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2022-04-29 19:36:01--  https://raw.githubusercontent.com/raghadt/Generating-Lyrics/master/LDR_lyrics.json\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 547793 (535K) [text/plain]\n","Saving to: ‘LDR_lyrics.json’\n","\n","LDR_lyrics.json     100%[===================>] 534.95K  --.-KB/s    in 0.04s   \n","\n","2022-04-29 19:36:02 (13.2 MB/s) - ‘LDR_lyrics.json’ saved [547793/547793]\n","\n"]}],"source":["!wget https://raw.githubusercontent.com/raghadt/Generating-Lyrics/master/LDR_lyrics.json"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1651260962118,"user":{"displayName":"Raghad Taleb","userId":"09571408088199853670"},"user_tz":-60},"id":"o3ZSf4UdPyni"},"outputs":[],"source":["#@title Importing Data { form-width: \"30%\" }\n","\n","# data_path = '/content/LDR_lyrics.json'  #@param {type: \"string\"}\n","\n","# Open the data in the path\n","with open('LDR_lyrics.json', 'r') as f:\n","  lana_lyrics = json.load(f)\n","\n","# Join the the songs lyrics into one string value\n","lyrics = \"\"\n","lyrics_array=[]\n","for song in lana_lyrics:\n","  lyrics+= lana_lyrics[song] + '\\n'\n","  # lyrics_array.append(lana_lyrics[i])\n","  \n"]},{"cell_type":"markdown","metadata":{"id":"iVcJNL6fBkt8"},"source":["lana_lyrics: \n","ultraviolence: '....',\n","borntodie: '....'\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1651260962118,"user":{"displayName":"Raghad Taleb","userId":"09571408088199853670"},"user_tz":-60},"id":"T9deHgTZBh3i"},"outputs":[],"source":["def exclude_char(st,execluded_chars):\n","  clean_st = ''\n","  for char in st:\n","    if char not in execluded_chars:\n","      clean_st+=char\n","  return clean_st"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":208,"status":"ok","timestamp":1651260962323,"user":{"displayName":"Raghad Taleb","userId":"09571408088199853670"},"user_tz":-60},"id":"OOIy_XPNEH-i","outputId":"15b54669-9a90-47d0-f2c7-ffd9b1ae65c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of unique characters: 77 \n"]}],"source":["#@title Data Cleaning { form-width: \"30%\" }\n","\n","\n","# text = text.replace('(','').replace(')','')\n","execluded_chars = '/!()*.:=[]«»;؛,،~?؟#\\u200f\\ufeff\\“\\\"\\'\\”'\n","\n","text = \"\"\n","\n","\n","#Here, we are taking only unique sentences from each song. \n","# the artist can have themes in their music, therefore it's not fair to take the unique sentences from the whole lyrics corpus\n","lyrics_array_sentence = []\n","#loop through the lyrics dictoinary\n","for song_name in lana_lyrics:\n","  song_sentences = []\n","  #split the song into lines. loop through each line in the song. \n","  for song_line in (lana_lyrics[song_name].split('\\n')):\n","    song_line = exclude_char(song_line,execluded_chars)\n","    song_sentences.append(song_line)\n","  #get the unique line of each song\n","  song_sentences = list(set(song_sentences))\n","  #add them set of lines into a big list\n","  lyrics_array_sentence.extend(song_sentences)\n","\n","# create a character set\n","for sentence in lyrics_array_sentence:\n","  for char in sentence:\n","    text += char\n","  text+='\\n'\n","\n","text = text.replace(\"\\r\\n\",\"\\n\")\n","text = text.replace(\"\\t\\t\\t\", \"\\t\")\n","text = text.replace(\"\\r\\r\\n\", \"\\n\")\n","text = text.replace(\"\\t\\n\", \"\\n\")\n","\n","#get the number of characters in a the lyrics\n","char_set = sorted(set(text))\n","print ('Number of unique characters: {} '.format(len(char_set)))"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":196,"status":"ok","timestamp":1651260974574,"user":{"displayName":"Raghad Taleb","userId":"09571408088199853670"},"user_tz":-60},"id":"6ShHOde4Ij2k"},"outputs":[],"source":["# char_set"]},{"cell_type":"markdown","metadata":{"id":"OoZWBb6BXC2O"},"source":["**Vectorize the text**\n","\n","Strings must be mapped to numerical representations. Two lookup tables are required: one to map characters to numbers, and the other to map numbers to characters.\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1651260974863,"user":{"displayName":"Raghad Taleb","userId":"09571408088199853670"},"user_tz":-60},"id":"9x1r2gFNXC2O"},"outputs":[],"source":["# create the 2 data structures, from characters to indices\n","char2idx = {u:i for i, u in enumerate(char_set)}\n","idx2char = np.array(char_set)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1651260975226,"user":{"displayName":"Raghad Taleb","userId":"09571408088199853670"},"user_tz":-60},"id":"D_KZZTwvEH-j","outputId":"7b9ac2ea-a058-49b6-8c2f-c01ae6b0559b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Batch size: 128       \n","Buffer size: 10000      \n","Number of RNN Units: 256      \n","Max input length: 50       \n","Vocabulary size: 77 \n","Embedding dimension: 2000\n"]}],"source":["#@title Dataset parameters { form-width: \"30%\" }\n","\n","\n","# batch size, default: 128\n","BATCH_SIZE = 128  # @param {type: \"integer\"}\n","\n","# buffer size is used to shuffle the dataset. it will attempt to shuffle the complete sqeuence in the memory, however, it will maintain a buffer that shuffles the elements\n","BUFFER_SIZE = 10000  # @param {type: \"integer\"}\n","\n","# number of RNN units #1024\n","N_RNN_UNITS = 256   # @param {type: \"integer\"}\n","\n","# length of text chunks for training, default 100\n","MAX_LENGTH = 50  # @param {type: \"integer\"}\n","\n","# the embedding dimensions256\n","EMBEDDING_DIM =  2000   # @param {type: \"integer\"}\n","\n","# the length of the vocabulary in chars\n","VOCAB_SIZE = len(char_set)\n","\n","print(\"Batch size: {} \\\n","      \\nBuffer size: {}\\\n","      \\nNumber of RNN Units: {}\\\n","      \\nMax input length: {} \\\n","      \\nVocabulary size: {} \\nEmbedding dimension: {}\".format(\n","            BATCH_SIZE, BUFFER_SIZE, N_RNN_UNITS, MAX_LENGTH, VOCAB_SIZE, EMBEDDING_DIM\n","        )\n",")"]},{"cell_type":"markdown","metadata":{"id":"9wIOCZKYXC2Q"},"source":["**Creating Training Examples and Targets for the Model**\n","Now, we need to segment the text into example sequences. There are *seq_length* characters in each input sequence. The corresponding targets in each input sequence are the same length as the input sequence, except shifted one character to the right.\n","Example: \n","- Original Text: \"Ground control to major Tom\"\n","- Input: \"Ground control to major To\"\n","- Target: \"round control to major Tom\"\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1651260975451,"user":{"displayName":"Raghad Taleb","userId":"09571408088199853670"},"user_tz":-60},"id":"YaCUSDq5vWm9"},"outputs":[],"source":["#@title Obtaining input and target data\n","\n","\n","input_data = []\n","target_data = []\n","\n","for c in range(0, len(text)-MAX_LENGTH, MAX_LENGTH):\n","    inps = text[c : c + MAX_LENGTH]\n","    tars = text[c + 1 : c + 1 + MAX_LENGTH]\n","\n","    input_data.append([char2idx[i] for i in inps])\n","    target_data.append([char2idx[t] for t in tars])\n","\n","# print (np.array(input_data).shape)\n","# print (np.array(target_data).shape)"]},{"cell_type":"markdown","metadata":{"id":"NpxQ4PM0XC2R"},"source":["Now that we have our inpnut and target data, we will need to slice our original dataset to convert the text vector into a stream of indices representing the characters."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1621,"status":"ok","timestamp":1651260977524,"user":{"displayName":"Raghad Taleb","userId":"09571408088199853670"},"user_tz":-60},"id":"iTa4r7TIXC2R","outputId":"31407f93-623a-43b5-945f-b61230c0d060"},"outputs":[{"data":{"text/plain":["<BatchDataset element_spec=(TensorSpec(shape=(128, 50), dtype=tf.int32, name=None), TensorSpec(shape=(128, 50), dtype=tf.int32, name=None))>"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["#@title Batch datasets\n","\n","#split the original dataset into sequences, we must shuffle and pack  the data as batches before feeding it into the model\n","dataset = tf.data.Dataset.from_tensor_slices((input_data, target_data)).shuffle(BUFFER_SIZE)\n","\n","# convert characters to sequences of batch size we pre-defined\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","\n","dataset\n"]},{"cell_type":"markdown","metadata":{"id":"cVlqA_zWy2TO"},"source":["-------\n","# Building the Network\n","\n","The model will be trained to try and predict \"What is most likely to follow a certain character, or a sequence of characters? \". This way the model can generate text depending on the style of the data it is given.\n","\n","We train the model to predict the sequence of characters we preprocessed already, and each time step the model will be fed a new input sequence.\n","\n","Since RNNs have an internal state that is based on the previous elements, to try and predicit what is the next character, given all the characters computed so far.\n","\n","To build the model, we will use  `tf.keras.Sequential` as a base for the model.\n","3 different layers will be used:\n","\n","* `tf.keras.layers.Embedding`: The input layer. This is a trainable lookup table to map the numbers of each character to a vector with the embedding dimensions: `embedding_dim`\n","* `tf.keras.layers.GRU`:GRU is a type of RNN. we will use the pre-defined number of RUNN units as a size `units=rnn_units`\n","* `tf.keras.layers.Dense`: The output layer. it will provide outputs with `vocab_size` size.\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":520,"status":"ok","timestamp":1651260983754,"user":{"displayName":"Raghad Taleb","userId":"09571408088199853670"},"user_tz":-60},"id":"n4TZO1ddzYYr","outputId":"c45bd97f-b007-44df-dcc4-20bad87bbab9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (128, None, 2000)         154000    \n","                                                                 \n"," lstm (LSTM)                 (128, None, 256)          2311168   \n","                                                                 \n"," dense (Dense)               (128, None, 77)           19789     \n","                                                                 \n","=================================================================\n","Total params: 2,484,957\n","Trainable params: 2,484,957\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["#@title Set up generator network structure { form-width: \"30%\" }\n","\n","# Define input and output around the RNN (GRU)\n","def build_model(vocab_size=VOCAB_SIZE, embedding_dim=EMBEDDING_DIM, n_rnn_units=N_RNN_UNITS, batch_size=BATCH_SIZE):\n","    # we will use tf.keras.Sequential to define the model, the following layers are used to construct the sequential model:\n","    model = tf.keras.Sequential([\n","          #Embedding layer is an input layer. This is a trainable lookup table to map the numbers of each character to a vector with the embedding dimensions;               \n","            tf.keras.layers.Embedding(vocab_size, embedding_dim,\n","                                      batch_input_shape=[batch_size, None]),\n","            # LSTM is a type of RNN. we will use the pre-defined number of RUNN units as a size\n","            tf.keras.layers.LSTM(n_rnn_units,\n","                                return_sequences=True,\n","                                stateful=True,\n","                                recurrent_activation='sigmoid',\n","                                recurrent_initializer='glorot_uniform'\n","                                ),\n","            # finally, the  Dense layer is the output layer, with VOCAB_SIZE as outputs.\n","            tf.keras.layers.Dense(vocab_size)\n","        ])\n","    model.summary()\n","    return model\n","\n","model = build_model()"]},{"cell_type":"markdown","metadata":{"id":"ujQ3UuQfXC2T"},"source":["Define the parameters of the model, loss function, learning rate"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":202,"status":"ok","timestamp":1651260985568,"user":{"displayName":"Raghad Taleb","userId":"09571408088199853670"},"user_tz":-60},"id":"UtU4uyS7XC2U"},"outputs":[],"source":["# Define the loss function\n","def loss_function(labels, logits):\n","    # categorical cross entropy is used because it is applied across the last dimension of the predictions.\n","    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n","\n","\n","# Define the optimiser\n","# learning rate will influence the model outcome. learning rate is a parameter that can be tuned to optimize the model, \n","# and it represents how big of a step the model is taking. We can change learning rate from the parameters of the model.\n","learning_rate = 0.001  #@param{type:\"raw\"}\n","\n","#We will use Adam optimizer. Although Adam is generally considered to be relatively robust to the choice of hyperparameters.\n","# it is sometimes necessary to change the learning rate from the default value.\n","optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.5)\n","\n","# Compile the model\n","model.compile(optimizer, loss_function)\n"]},{"cell_type":"markdown","metadata":{"id":"AkB9QkweXC2V"},"source":["The output distribution of the model needs to be sampled to get actual character indices. The logits of the vocabulary describe the output distribution. To avoid getting stuck in a loop, it is important to select samples from this distribution. Taking the argmax of the distribution can easily cause the model to become stuck.\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":928,"status":"ok","timestamp":1651260988167,"user":{"displayName":"Raghad Taleb","userId":"09571408088199853670"},"user_tz":-60},"id":"3A1Psm8jzfuQ","outputId":"74d301c4-379c-4cd4-aba5-3f52b5676d87"},"outputs":[{"name":"stdout","output_type":"stream","text":["(128, 50, 77) # (batch_size, sequence_length, vocab_size)\n","Input: \n"," 'danger\\nFor the sixth page\\nI can tell I can tell\\nMo' \n","\n","Next Char Predictions: \n"," 'lABt7vNQ5órxàolUTjô1œ9Vr4tocz  zdfWNmsHîp70ROUkvz–'\n"]}],"source":["#@title Test configuration with one example { form-width: \"30%\" }\n","\n","for input_example_batch, target_example_batch in dataset.take(1):\n","    # Run the batch through the model\n","    example_batch_predictions = model(input_example_batch)\n","\n","    # sample over the output distribution to get an actual prediction from the model.\n","    # The distribution is defined by the logits over the characters vocabs\n","    sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n","    sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy() \n","    \n","    # Double check the shape of the output\n","    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n","\n","    # Decode the indices to see the text predicted by the (untrained) model\n","    print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])), \"\\n\")\n","    print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"]},{"cell_type":"markdown","metadata":{"id":"JLbjfRXjEH-l"},"source":["## Training The Model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OuHA7ESpHfdr"},"outputs":[],"source":["# #@title Save checkpoints during training { form-width: \"30%\" }\n","\n","# from google.colab import drive\n","# drive.mount('/content/gdrive')\n","\n","# # Directory where the checkpoints will be saved\n","# path = 'My Drive/CC_CW_v3/checkpoint/' #@param{type: 'string'}\n","# full_path = \"/content/gdrive/\" + path + \"ckpt_{epoch}\" \n","\n","# checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","#                       filepath=full_path,\n","#                       save_weights_only=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wydDzGdmHhPh"},"outputs":[],"source":["#@title Train the model { form-width: \"30%\" }\n","\n","# default: 10\n","n_epochs =  100# @param{type: \"integer\"} \n","# history = model.fit(dataset, epochs=n_epochs, callbacks=[checkpoint_callback])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_P-lAQ0ZwswF"},"outputs":[],"source":["# from plot_keras_history import plot_history\n","# import matplotlib.pyplot as plt\n","# plot_history(history.history, path=\"standard.png\")\n","# plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ZX1BS_K7XC2X"},"source":["-----\n","## Generate Text\n","For the text generation part, we will use a batch size of one for this prediction step to keep things simple since a fixed batch size is required once the model has been built due to the way RNN state is passed from timestep to timestep. We will need to restore the model that we already trained and saved, rebuild it and restore weights from the checkpoints. \n","\n","As a start, the model selects a start string based on the sent parameter, then initializes the RNN state, and sets the number of characters to generate. we will use the start string and the RNN state to determine the prediction distribution of the next character.\n","\n","Following that, we will calculate the predicted character's index using a multinomial distribution. We will use the predicted character as our next input.\n","\n","RNN state is returned by the model so more context can be added to the model instead of only one word. \n","The model learns as it gets more context by seeing the previous predicted words and by providing the modified RNN states again. The model picks up on details like capitalization, paragraphs and emphases through the generated text.\n","\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":172,"status":"ok","timestamp":1651260990315,"user":{"displayName":"Raghad Taleb","userId":"09571408088199853670"},"user_tz":-60},"id":"HML7_661AXIH"},"outputs":[],"source":["#@title Set up text generation function { form-width: \"30%\" }\n","\n","def generate_text(model, input_data, n_characters_output=1000):\n","    # First, vectorize the input text as before\n","    input_eval = [char2idx[s] for s in input_data]\n","    input_eval = tf.expand_dims(input_eval, 0)\n","\n","    # We'll store results in this variable\n","    text_generated = []\n","\n","    # Generate the number of characters desired\n","    model.reset_states()\n","    for i in range(n_characters_output):\n","        # Run input through model\n","        predictions = model(input_eval)\n","\n","        # Remove the batch dimension\n","        predictions = tf.squeeze(predictions, 0)\n","\n","        # Using a categorical distribution to predict the character returned by the model\n","        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n","\n","        # Pass the predicted character as the next input to the model\n","        input_eval = tf.expand_dims([predicted_id], 0)\n","\n","        # Add the predicted character to the output\n","        text_generated.append(idx2char[predicted_id])\n","\n","    # Return output\n","    return (input_data + ''.join(text_generated))"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5010,"status":"ok","timestamp":1651260998992,"user":{"displayName":"Raghad Taleb","userId":"09571408088199853670"},"user_tz":-60},"id":"cokzMwYtsNID","outputId":"e3a89a49-441b-4eb9-d17a-480672043f0a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.6.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n","Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n","/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  category=FutureWarning,\n","Downloading...\n","From: https://drive.google.com/uc?id=1eIaMWKHaF7uswUDzlWXIi7ChRsaij9aO\n","To: /content/checkpoint_100.zip\n","100% 27.7M/27.7M [00:00<00:00, 74.3MB/s]\n"]}],"source":["# # download checkpoints\n","# !wget https://drive.google.com/uc?id=1eBR2RHOmaNUV93G5qb13EHY9LygEbUi2\n","!pip install gdown\n","!gdown --id 1eIaMWKHaF7uswUDzlWXIi7ChRsaij9aO\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":654,"status":"ok","timestamp":1651261003705,"user":{"displayName":"Raghad Taleb","userId":"09571408088199853670"},"user_tz":-60},"id":"JkaB3e7ey8k-","outputId":"43288481-d7e4-4a68-9d0d-021211139b88"},"outputs":[{"name":"stdout","output_type":"stream","text":["Archive:  checkpoint_100.zip\n","   creating: checkpoint_100/\n","  inflating: __MACOSX/._checkpoint_100  \n","  inflating: checkpoint_100/ckpt_100.index  \n","  inflating: __MACOSX/checkpoint_100/._ckpt_100.index  \n","  inflating: checkpoint_100/ckpt_100.data-00000-of-00001  \n","  inflating: __MACOSX/checkpoint_100/._ckpt_100.data-00000-of-00001  \n","  inflating: checkpoint_100/checkpoint  \n","  inflating: __MACOSX/checkpoint_100/._checkpoint  \n"]}],"source":["!unzip checkpoint_100.zip"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":197,"status":"ok","timestamp":1651261008131,"user":{"displayName":"Raghad Taleb","userId":"09571408088199853670"},"user_tz":-60},"id":"2vGM3ex0zH0d","outputId":"db8f7b89-d246-450e-f6dd-c198ad6f7d73"},"outputs":[{"name":"stdout","output_type":"stream","text":["checkpoint_100\tcheckpoint_100.zip  LDR_lyrics.json  __MACOSX  sample_data\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":273,"status":"ok","timestamp":1651261009003,"user":{"displayName":"Raghad Taleb","userId":"09571408088199853670"},"user_tz":-60},"id":"fuwBgIIJHl9l","outputId":"d3acd00f-160d-4338-8c6d-47d81d29c752"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (1, None, 2000)           154000    \n","                                                                 \n"," lstm_1 (LSTM)               (1, None, 256)            2311168   \n","                                                                 \n"," dense_1 (Dense)             (1, None, 77)             19789     \n","                                                                 \n","=================================================================\n","Total params: 2,484,957\n","Trainable params: 2,484,957\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["#@title Restore latest checkpoint and build model { form-width: \"30%\" }\n","\n","# path = 'My Drive/Work/Colab/TextGen/' #@param{type: 'string'}\n","# path = 'My Drive/CC_CW_v3/checkpoint_100/' #@param{type: 'string'}\n","batch_size = 1\n","\n","model = build_model(batch_size=batch_size)\n","# model.load_weights(tf.train.latest_checkpoint(\"/content/gdrive/\" + path))\n","model.load_weights(tf.train.latest_checkpoint(\"checkpoint_100\"))\n","\n","\n","model.build(tf.TensorShape([batch_size, None]))"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7117,"status":"ok","timestamp":1651261017886,"user":{"displayName":"Raghad Taleb","userId":"09571408088199853670"},"user_tz":-60},"id":"SnhQgxjL6UQB","outputId":"f71b75a0-a931-4b10-ab35-71e8d679e773"},"outputs":[{"name":"stdout","output_type":"stream","text":["happy bars\n","What happick and roller\n","Got youre big stristenders are between from your pow\n","Its just a pin-yes you\n","Its like I never knew do\n","To runs we have you\n","\n","Soul not sad\n","\n","Oh Oh oh oh oh oh-oh oh\n","Yeah oh Yeah oh\n","Dont you stay\n","Pretty dreams that\n","And I want is is a prize come to the eitch just for real for reason with you\n","Was only since you dont like you I heard and push on\n","\n","\n","Come on back to my neise\n","I got you I wanna go tappem\n","I cant Rall\n","Its go Lany on your head can untup baby you I like my baby bout screess Can I see to leave\n","\n","Baby youre a dark so bad Nos gone Monight line but high Wailia roller\n","The wind Im gone cause were going\n","Bereaking imates Cypaties\n","One day is it forever\n","Hey boo lets go back on the promised\n","I can and e\n","Can you like fine man jive\n","I said Baby Youre so hot courser they\n","Even then I like to keep that way\n","Breaking up radit off me\n","You dont wanna diate from got that one-top of hard to sleep like smile and young youre the end them\n","Ooh oh\n","Theres no pray I sight But to pread\n","On si\n"]}],"source":["#@title Generate text! { form-width: \"30%\" }\n","\n","input_data = \"happy \"  # @param {type: \"string\"}\n","n_characters_output = 1000 #@param \n","# print(generate_text(model, input_data=input_data, n_characters_output=n_characters_output))\n","print(generate_text(model, input_data,n_characters_output))\n"]},{"cell_type":"markdown","metadata":{"id":"TA8hl-P-pSJV"},"source":["------\n","# Markovian Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4280,"status":"ok","timestamp":1651260676975,"user":{"displayName":"Raghad Taleb","userId":"09571408088199853670"},"user_tz":-60},"id":"7SHqWW79pPUK","outputId":"296f42b2-004d-4901-da75-df862f3d7722"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting markovify\n","  Downloading markovify-0.9.4.tar.gz (27 kB)\n","Collecting unidecode\n","  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n","\u001b[K     |████████████████████████████████| 235 kB 7.8 MB/s \n","\u001b[?25hBuilding wheels for collected packages: markovify\n","  Building wheel for markovify (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for markovify: filename=markovify-0.9.4-py3-none-any.whl size=18628 sha256=b0bc245fc832936f41688346e57dc436df7641655bdd350716c16fbef677f016\n","  Stored in directory: /root/.cache/pip/wheels/36/c5/82/11125c5a7dadec27ef49ac2b3a12d3b1f79ff7333c92a9b67b\n","Successfully built markovify\n","Installing collected packages: unidecode, markovify\n","Successfully installed markovify-0.9.4 unidecode-1.3.4\n"]}],"source":["!pip install markovify"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"skY1d8UaHos4"},"outputs":[],"source":["import markovify"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fn27rPcjrdml"},"outputs":[],"source":["# len(set(all_lines))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QHYj2W5oplDI"},"outputs":[],"source":["all_lines = list(set(lyrics_array_sentence))\n","big_poem = \"\\n\".join([line for line in random.sample(all_lines, len(all_lines))])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aguudG0sqBU8"},"outputs":[],"source":["model = markovify.NewlineText(big_poem)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1651260677263,"user":{"displayName":"Raghad Taleb","userId":"09571408088199853670"},"user_tz":-60},"id":"sLlHFEnHqBcS","outputId":"3660c4a2-fc34-404e-da6f-cec64263d32c"},"outputs":[{"name":"stdout","output_type":"stream","text":["I didnt believe you when we booed you off easy its alright\n","Its just that good\n","Pin up girls at the movie stars\n","But you the king of the scale of the sea hands all over now\n","How about about the things youve done to me everything you got so scared I thought\n","Be proud come out with a song\n","The Queen of Saigon\n","But in my hair up\n","I still love me youll love me hardcore then dont walk away\n","Did you want a little song\n","You know Im wrong\n","The kind of people you meet on the run with you my king\n","Crying cause I like it when its me\n","Mary swaying softly to her bare ass\n"]}],"source":["for i in range(14):\n","    print(model.make_sentence())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DAz6QleEshZ4"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"3-v1l1AfqbUS"},"source":["# Comparing Results\n","Assumption: generated text will most likly contain the most repeatitive words of the data corpus.\n","\n","we will remove stop words since they're most like to be repeated"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CoS3AY6uQcf4"},"outputs":[],"source":["import re"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":178,"status":"ok","timestamp":1651234812764,"user":{"displayName":"Raghad Taleb","userId":"09571408088199853670"},"user_tz":-60},"id":"zryhiiUQqF2P","outputId":"6c0d1232-2801-4960-e9fa-c728f8340029"},"outputs":[{"name":"stdout","output_type":"stream","text":["I had bright wishes in the summer I was bathing in sunlight\n","Its the middle of winter but in my mind its summer Mike\n","High tops in the summer Summer what What What\n","High tops in the summer Top out hop out hop out\n","Hip-hop in the summer What What What\n","Hot summer and cold watermelon\n","Hot like the summer and mean like a child\n","Hear the birds on the summer breeze\n","Like a summer breeze\n","I hear the birds on the summer breeze\n"]}],"source":["word = 'summer'\n","searched_lines = sorted(\n","    [line for line in all_lines if re.search(r\"\\bsummer\\b\\s\\w\", line)], \n","    key=lambda line: line[re.search(r\"\\bsummer\\b\\s\", line).end():]) \n","\n","for line in searched_lines[0:10]:\n","    print(line)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4p15eopyQ8Q-"},"outputs":[],"source":["from collections import Counter\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords\n","import nltk\n","import string"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1651235113962,"user":{"displayName":"Raghad Taleb","userId":"09571408088199853670"},"user_tz":-60},"id":"pWFDMNG2R-b_","outputId":"9d01dd5f-e957-4cf3-eb3d-49394e493782"},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["nltk.download('stopwords')\n","#define stop words, using set to get a unique set.\n","stop = set(stopwords.words('english') + list(string.punctuation))"]},{"cell_type":"markdown","metadata":{"id":"6oHlz9DZRWnf"},"source":["**Finding most common words**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DLzKIvpNRdFe"},"outputs":[],"source":["# words_list = text.replace('\\n',' ').split(' ')\n","words_list = []\n","for word in text.replace('\\n',' ').split(' '):\n","  if (word.lower() not in stop):\n","    words_list.append(word.lower())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1651235225506,"user":{"displayName":"Raghad Taleb","userId":"09571408088199853670"},"user_tz":-60},"id":"TDcMWhaDRBhq","outputId":"72ed4400-6f56-4bcb-b231-81ea9479acf4"},"outputs":[{"data":{"text/plain":["[('im', 823),\n"," ('like', 794),\n"," ('', 673),\n"," ('baby', 494),\n"," ('dont', 493),\n"," ('know', 437),\n"," ('love', 437),\n"," ('youre', 429),\n"," ('got', 309),\n"," ('get', 293),\n"," ('want', 265),\n"," ('say', 235),\n"," ('oh', 232),\n"," ('cause', 227),\n"," ('wanna', 218),\n"," ('one', 212),\n"," ('go', 209),\n"," ('never', 203),\n"," ('come', 197),\n"," ('way', 189),\n"," ('make', 173),\n"," ('yeah', 172),\n"," ('think', 170),\n"," ('life', 164),\n"," ('take', 157),\n"," ('see', 153),\n"," ('time', 152),\n"," ('cant', 151),\n"," ('back', 147),\n"," ('said', 147)]"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["Counter(words_list).most_common(30)"]},{"cell_type":"markdown","metadata":{"id":"b5L6s9F5SiTn"},"source":["we observe that **love, think, life, time** are very repeative, now let's see if they are repetitive as well in our text generated text\n","NOTE: I took the text that was used for evaluting the model. that's why it's hard coded"]},{"cell_type":"markdown","metadata":{"id":"1z0rLjoR_iNN"},"source":["**RNN Model**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":187,"status":"ok","timestamp":1651235779894,"user":{"displayName":"Raghad Taleb","userId":"09571408088199853670"},"user_tz":-60},"id":"7oFii2VdSs9Y","outputId":"12d76540-532d-44e7-e0e1-871849889a05"},"outputs":[{"name":"stdout","output_type":"stream","text":["('love', 1)\n"]}],"source":["RNN_text= \"happytrawsoars that Imbana Jimmy pring Or your waver I conty up now you do it doesnt know come Used to love to Cheaver fasorio let theres no you loved morning They she doice me langer egas car wrice new scregiase We gonna say that the what do you here The world is deadd Im lonely goodbybody so You gonna learny sce and I can take it all you Swinging in my dress Learn me just you come Youll be appeat I smoke until we were born that the world But dont get you it enough Glamorou ride are run run awour hemelt me fresh that on the man yeah Hm the man when you call God knows I lose your little caoset otch Back around when he smoking now And Ilsed get her Youre lying theresome the count you should be bad Ats not to heart Oh-oh-oh-oh-h-h State his golden dangeresser To see her making me like sheeper Babe its alright Why Mayo believe in my disant streets But honest my sw He wre lying in the run Im beliou still get out laughing Dead Im doing no to lose at elseamanost daddy Least your flection Oh dont\"\n","RNN_words_list = []\n","for word in RNN_text.split(' '):\n","  if (word.lower() not in stop):\n","    RNN_words_list.append(word.lower())\n","\n","for i in Counter(RNN_words_list).most_common():\n","  # print(j)\n","  if i[0] == 'love' or i[0] =='life' or i[0] =='think' or i[0] =='time' or i[0] =='i':\n","    print(i)\n"]},{"cell_type":"markdown","metadata":{"id":"Mh3zeBhi_kiH"},"source":["**Markovian Model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YC3V9lv7Uouy"},"outputs":[],"source":["mark_text= \"LA I dont pop my cork for every experience and an obsession for freedom that terrified me to the Beach Boys He dont like no one which means theres only truth and God is playing a little bit of paradise Keep me safe in the sky with ice tonight Im lying in the sand Anything you like you were seeing yeah You said I know thats not a school girl crush You cant be with the blue mascara You should never ever stop No Make me smile and your gun Even if youre able They call you up twice hang up the H of the Gods Smoking all my money gave you all the ways The secrets you keep it moving babe Ill do what you desire\"\n","\n","mark_words_list = []\n","for word in mark_text.split(' '):\n","  if (word.lower() not in stop):\n","    mark_words_list.append(word.lower())\n","\n","for i in Counter(mark_words_list).most_common():\n","  # print(j)\n","  if i[0] == 'love' or i[0] =='life' or i[0] =='think' or i[0] =='time' or i[0] =='i':\n","    print(i)\n"]},{"cell_type":"markdown","metadata":{"id":"GgLI-0c75osP"},"source":["## Appendix  - Resources :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qfK58By05osQ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WQu5r39_5osQ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ADykvfyq5osR"},"outputs":[],"source":[]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"210885240_ECS7022P_Coursework.ipynb","provenance":[{"file_id":"1f8zY4b4YXBavE1d1gJQjPJ6q_HAf_lFd","timestamp":1649526529634},{"file_id":"1G7obuJalYQFFgumCi5y4WtNMzdJ5icEs","timestamp":1648466673054}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":0}
